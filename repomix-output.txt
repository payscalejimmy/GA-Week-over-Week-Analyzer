This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
app.py
README.md
requirements.txt

================================================================
Files
================================================================

================
File: .gitignore
================
*.csv
output/

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

================
File: app.py
================
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class GA4WeekOverWeekAnalyzer:
    def __init__(self, csv_path, output_dir='output'):
        """Initialize the analyzer with CSV path and output directory."""
        self.csv_path = csv_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.df = None
        self.weekly_data = {}
        
    def load_data(self):
        """Load CSV data starting from row 7 (header) and skip row 8 (grand total)."""
        print("Loading data...")
        
        # Skip comment rows (0-5) AND the grand total row (7)
        # This uses row 6 (row 7 in 1-indexed, the header row) as the header
        # and starts reading data from row 8 (row 9 in 1-indexed)
        rows_to_skip = list(range(6)) + [7]  # Skip rows 0-5 and row 7 (grand total)
        self.df = pd.read_csv(self.csv_path, skiprows=rows_to_skip, header=0)
        
        # Debug: Check what we loaded
        print(f"Initial shape: {self.df.shape}")
        print(f"Columns: {self.df.columns.tolist()}")
        
        # Clean column names
        self.df.columns = self.df.columns.str.strip()
        
        # Reset index
        self.df = self.df.reset_index(drop=True)
        
        # Debug: Check Date column values before conversion
        print(f"\nDate column dtype: {self.df['Date'].dtype}")
        print(f"First 5 date values: {self.df['Date'].head().tolist()}")
        
        # Convert date column - handle both string and numeric formats
        if self.df['Date'].dtype == 'object':
            # It's already a string
            self.df['Date'] = pd.to_datetime(self.df['Date'], format='%Y%m%d', errors='coerce')
        else:
            # It's numeric, convert to string first
            self.df['Date'] = self.df['Date'].astype(int).astype(str)
            self.df['Date'] = pd.to_datetime(self.df['Date'], format='%Y%m%d', errors='coerce')
        
        print(f"Dates after conversion - valid: {self.df['Date'].notna().sum()}, invalid: {self.df['Date'].isna().sum()}")
        
        # Remove any rows with invalid dates
        initial_rows = len(self.df)
        self.df = self.df.dropna(subset=['Date']).copy()
        print(f"Removed {initial_rows - len(self.df)} rows with invalid dates")
        
        # Convert numeric columns
        numeric_cols = ['Total users', 'Engagement rate', 'Key events', 'User key event rate']
        for col in numeric_cols:
            if col in self.df.columns:
                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
        
        print(f"\nLoaded {len(self.df)} rows of data")
        if len(self.df) > 0:
            print(f"Date range: {self.df['Date'].min()} to {self.df['Date'].max()}")
            print(f"\nSample of loaded data:")
            print(self.df[['Session Payscale Custom Channels', 'Date', 'Total users', 'Key events']].head(3))
        else:
            print("ERROR: No valid data rows loaded!")
        
    def create_weekly_groups(self):
        """Group data by week."""
        print("\nGrouping data by week...")
        
        # Add week number (Monday as start of week)
        self.df['Week'] = self.df['Date'].dt.to_period('W-SUN')
        self.df['Week_Start'] = self.df['Week'].apply(lambda x: x.start_time)
        
        # Get unique weeks sorted
        weeks = sorted(self.df['Week_Start'].unique())
        
        print(f"Found {len(weeks)} weeks:")
        for i, week in enumerate(weeks, 1):
            week_end = week + timedelta(days=6)
            print(f"  Week {i}: {week.strftime('%Y-%m-%d')} to {week_end.strftime('%Y-%m-%d')}")
        
        return weeks
    
    def check_missing_dates(self, weeks):
        """Check for missing dates within each week's range."""
        print("\nChecking for missing dates...")
        
        missing_dates_by_week = {}
        
        for i, week in enumerate(weeks, 1):
            week_end = week + timedelta(days=6)
            
            # Get all dates that should be in this week
            expected_dates = pd.date_range(start=week, end=week_end, freq='D')
            
            # Get actual dates in the data for this week
            actual_dates = self.df[self.df['Week_Start'] == week]['Date'].dt.date.unique()
            actual_dates = pd.to_datetime(actual_dates)
            
            # Find missing dates
            missing = [d for d in expected_dates if d not in actual_dates]
            
            if missing:
                missing_dates_by_week[i] = {
                    'week_start': week,
                    'week_end': week_end,
                    'missing_dates': missing
                }
                print(f"  Week {i}: Missing {len(missing)} date(s) - {', '.join([d.strftime('%Y-%m-%d') for d in missing])}")
            else:
                print(f"  Week {i}: Complete (all 7 days present)")
        
        return missing_dates_by_week
    
    def aggregate_weekly_data(self, group_by_col, weeks):
        """Aggregate data by week for a specific grouping column."""
        weekly_agg = []
        
        for week in weeks:
            week_data = self.df[self.df['Week_Start'] == week].copy()
            
            if len(week_data) == 0:
                continue
            
            grouped = week_data.groupby(group_by_col).agg({
                'Total users': 'sum',
                'Key events': 'sum',
                'Engagement rate': 'mean',
                'User key event rate': 'mean'
            }).reset_index()
            
            grouped['Week_Start'] = week
            weekly_agg.append(grouped)
        
        if weekly_agg:
            return pd.concat(weekly_agg, ignore_index=True)
        return pd.DataFrame()
    
    def calculate_week_over_week(self, df, weeks, group_col):
        """Calculate week-over-week changes."""
        comparisons = []
        
        for i in range(len(weeks) - 1):
            current_week = weeks[i + 1]
            previous_week = weeks[i]
            
            current_data = df[df['Week_Start'] == current_week].copy()
            previous_data = df[df['Week_Start'] == previous_week].copy()
            
            # Merge on the grouping column
            merged = current_data.merge(
                previous_data,
                on=group_col,
                suffixes=('_current', '_previous'),
                how='outer'
            )
            
            # Fill NaN values with 0 for calculations
            merged = merged.fillna(0)
            
            # Calculate changes
            merged['Users_Change'] = merged['Total users_current'] - merged['Total users_previous']
            merged['Users_Change_Pct'] = np.where(
                merged['Total users_previous'] > 0,
                ((merged['Total users_current'] - merged['Total users_previous']) / 
                 merged['Total users_previous'] * 100),
                np.where(merged['Total users_current'] > 0, 100, 0)
            )
            
            merged['Key_Events_Change'] = merged['Key events_current'] - merged['Key events_previous']
            merged['Key_Events_Change_Pct'] = np.where(
                merged['Key events_previous'] > 0,
                ((merged['Key events_current'] - merged['Key events_previous']) / 
                 merged['Key events_previous'] * 100),
                np.where(merged['Key events_current'] > 0, 100, 0)
            )
            
            merged['Engagement_Change'] = (merged['Engagement rate_current'] - 
                                          merged['Engagement rate_previous'])
            
            merged['Week_Comparison'] = f"Week {i+2} vs Week {i+1}"
            merged['Current_Week'] = current_week
            merged['Previous_Week'] = previous_week
            
            comparisons.append(merged)
        
        if comparisons:
            return pd.concat(comparisons, ignore_index=True)
        return pd.DataFrame()
    
    def generate_channel_report(self, weeks):
        """Generate channel analysis report."""
        print("\nGenerating channel analysis...")
        
        channel_weekly = self.aggregate_weekly_data('Session Payscale Custom Channels', weeks)
        channel_wow = self.calculate_week_over_week(
            channel_weekly, weeks, 'Session Payscale Custom Channels'
        )
        
        # Save detailed CSV
        output_file = self.output_dir / 'channels_week_over_week.csv'
        channel_wow.to_csv(output_file, index=False)
        print(f"Saved: {output_file}")
        
        return channel_wow
    
    def generate_source_medium_report(self, weeks):
        """Generate source/medium analysis report."""
        print("Generating source/medium analysis...")
        
        sm_weekly = self.aggregate_weekly_data('Session source / medium', weeks)
        sm_wow = self.calculate_week_over_week(
            sm_weekly, weeks, 'Session source / medium'
        )
        
        # Save detailed CSV
        output_file = self.output_dir / 'source_medium_week_over_week.csv'
        sm_wow.to_csv(output_file, index=False)
        print(f"Saved: {output_file}")
        
        return sm_wow
    
    def generate_landing_page_report(self, weeks):
        """Generate landing page analysis report."""
        print("Generating landing page analysis...")
        
        lp_weekly = self.aggregate_weekly_data('Page path and screen class', weeks)
        lp_wow = self.calculate_week_over_week(
            lp_weekly, weeks, 'Page path and screen class'
        )
        
        # Save detailed CSV
        output_file = self.output_dir / 'landing_pages_week_over_week.csv'
        lp_wow.to_csv(output_file, index=False)
        print(f"Saved: {output_file}")
        
        return lp_wow
    
    def generate_landing_page_source_report(self, weeks):
        """Generate landing page + source/medium combination analysis."""
        print("Generating landing page + source/medium analysis...")
        
        # Create combined grouping column
        self.df['LP_Source'] = self.df['Page path and screen class'] + ' | ' + self.df['Session source / medium']
        
        lp_source_weekly = self.aggregate_weekly_data('LP_Source', weeks)
        lp_source_wow = self.calculate_week_over_week(
            lp_source_weekly, weeks, 'LP_Source'
        )
        
        # Save detailed CSV
        output_file = self.output_dir / 'landing_page_source_week_over_week.csv'
        lp_source_wow.to_csv(output_file, index=False)
        print(f"Saved: {output_file}")
        
        return lp_source_wow
    
    def generate_landing_page_channel_report(self, weeks):
        """Generate landing page + channel combination analysis."""
        print("Generating landing page + channel analysis...")
        
        # Create combined grouping column
        self.df['LP_Channel'] = self.df['Page path and screen class'] + ' | ' + self.df['Session Payscale Custom Channels']
        
        lp_channel_weekly = self.aggregate_weekly_data('LP_Channel', weeks)
        lp_channel_wow = self.calculate_week_over_week(
            lp_channel_weekly, weeks, 'LP_Channel'
        )
        
        # Save detailed CSV
        output_file = self.output_dir / 'landing_page_channel_week_over_week.csv'
        lp_channel_wow.to_csv(output_file, index=False)
        print(f"Saved: {output_file}")
        
        return lp_channel_wow
    
    def generate_executive_summary(self, channel_wow, sm_wow, lp_wow, lp_source_wow, lp_channel_wow, weeks, missing_dates_info):
        """Generate executive summary in Markdown format."""
        print("\nGenerating executive summary...")
        
        md_content = []
        md_content.append("# GA4 Week-over-Week Executive Summary")
        md_content.append(f"\n**Analysis Period:** {self.df['Date'].min().strftime('%B %d, %Y')} - {self.df['Date'].max().strftime('%B %d, %Y')}")
        md_content.append(f"\n**Report Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Add data completeness warning if there are missing dates
        if missing_dates_info:
            md_content.append("\n\n⚠️ **Data Completeness Notice:**\n")
            for week_num, info in missing_dates_info.items():
                missing_date_strs = [d.strftime('%b %d') for d in info['missing_dates']]
                md_content.append(f"- **Week {week_num}** ({info['week_start'].strftime('%b %d')} - {info['week_end'].strftime('%b %d')}): "
                                f"Missing data for {len(info['missing_dates'])} day(s) - {', '.join(missing_date_strs)}\n")
            md_content.append("\n*Note: Comparisons involving incomplete weeks should be interpreted with caution.*\n")
        
        md_content.append("\n---\n")
        
        # Overall metrics
        md_content.append("## Overall Performance\n")
        
        for i in range(len(weeks) - 1):
            week_comp = f"Week {i+2} vs Week {i+1}"
            md_content.append(f"### {week_comp}\n")
            md_content.append(f"**Period:** {weeks[i+1].strftime('%b %d')} - {(weeks[i+1] + timedelta(days=6)).strftime('%b %d')} vs {weeks[i].strftime('%b %d')} - {(weeks[i] + timedelta(days=6)).strftime('%b %d')}\n")
            
            # Add warning if either week in comparison has missing dates
            if (i+1) in missing_dates_info or (i+2) in missing_dates_info:
                md_content.append("\n⚠️ *This comparison includes incomplete week(s) - see Data Completeness Notice above.*\n")
            
            # NEW: Full Channel Performance Table
            channel_data = channel_wow[channel_wow['Week_Comparison'] == week_comp].copy()
            if not channel_data.empty:
                md_content.append("\n#### Complete Channel Performance Table\n")
                
                # Sort by users change (descending)
                channel_data = channel_data.sort_values('Users_Change', ascending=False)
                
                # Create markdown table
                md_content.append("| Channel | Current Week Users | Previous Week Users | User Change | User Change % | Current Week Key Events | Previous Week Key Events | Key Event Change | Key Event Change % |\n")
                md_content.append("|---------|-------------------:|--------------------:|------------:|--------------:|------------------------:|-------------------------:|-----------------:|-------------------:|\n")
                
                for _, row in channel_data.iterrows():
                    md_content.append(f"| {row['Session Payscale Custom Channels']} | "
                                    f"{row['Total users_current']:,.0f} | "
                                    f"{row['Total users_previous']:,.0f} | "
                                    f"{row['Users_Change']:+,.0f} | "
                                    f"{row['Users_Change_Pct']:+.1f}% | "
                                    f"{row['Key events_current']:,.0f} | "
                                    f"{row['Key events_previous']:,.0f} | "
                                    f"{row['Key_Events_Change']:+,.0f} | "
                                    f"{row['Key_Events_Change_Pct']:+.1f}% |\n")
                
                md_content.append("\n")
                
                # Keep the existing top/bottom highlights for quick scanning
                md_content.append("#### Top Channel Highlights\n")
                
                # Top gainers
                top_gainers = channel_data.head(3)
                md_content.append("**Biggest User Increases:**\n")
                for _, row in top_gainers.iterrows():
                    md_content.append(f"- **{row['Session Payscale Custom Channels']}**: "
                                    f"{row['Users_Change']:+,.0f} users "
                                    f"({row['Users_Change_Pct']:+.1f}%)\n")
                
                # Top decliners
                top_decliners = channel_data.tail(3).iloc[::-1]  # Reverse to show worst first
                md_content.append("\n**Biggest User Decreases:**\n")
                for _, row in top_decliners.iterrows():
                    md_content.append(f"- **{row['Session Payscale Custom Channels']}**: "
                                    f"{row['Users_Change']:+,.0f} users "
                                    f"({row['Users_Change_Pct']:+.1f}%)\n")
            
            # Source/Medium insights
            sm_data = sm_wow[sm_wow['Week_Comparison'] == week_comp]
            if not sm_data.empty:
                md_content.append("\n#### Top Source/Medium Changes\n")
                
                # Filter out rows with minimal activity
                sm_data_significant = sm_data[sm_data['Total users_current'] > 100]
                
                if not sm_data_significant.empty:
                    top_sm_gainers = sm_data_significant.nlargest(5, 'Users_Change')
                    md_content.append("**Biggest Traffic Increases:**\n")
                    for _, row in top_sm_gainers.iterrows():
                        md_content.append(f"- **{row['Session source / medium']}**: "
                                        f"{row['Users_Change']:+,.0f} users "
                                        f"({row['Users_Change_Pct']:+.1f}%) | "
                                        f"{row['Key events_current']:.0f} key events\n")
            
            # Landing page insights
            lp_data = lp_wow[lp_wow['Week_Comparison'] == week_comp]
            if not lp_data.empty:
                md_content.append("\n#### Top Landing Page Changes\n")
                
                # Filter out rows with minimal activity
                lp_data_significant = lp_data[lp_data['Total users_current'] > 50]
                
                if not lp_data_significant.empty:
                    top_lp_gainers = lp_data_significant.nlargest(5, 'Users_Change')
                    md_content.append("**Highest Traffic Growth Pages:**\n")
                    for _, row in top_lp_gainers.iterrows():
                        md_content.append(f"- `{row['Page path and screen class']}`: "
                                        f"{row['Users_Change']:+,.0f} users "
                                        f"({row['Users_Change_Pct']:+.1f}%)\n")
            
            # Landing Page + Source/Medium combinations
            lp_source_data = lp_source_wow[lp_source_wow['Week_Comparison'] == week_comp]
            if not lp_source_data.empty:
                md_content.append("\n#### Top Landing Page + Source/Medium Combinations\n")
                
                # Filter for significant traffic
                lp_source_significant = lp_source_data[lp_source_data['Total users_current'] > 50]
                
                if not lp_source_significant.empty:
                    top_combos = lp_source_significant.nlargest(5, 'Users_Change')
                    md_content.append("**Highest Growth Combinations:**\n")
                    for _, row in top_combos.iterrows():
                        parts = row['LP_Source'].split(' | ')
                        md_content.append(f"- **{parts[1]}** → `{parts[0]}`: "
                                        f"{row['Users_Change']:+,.0f} users "
                                        f"({row['Users_Change_Pct']:+.1f}%) | "
                                        f"{row['Key events_current']:.0f} conversions\n")
            
            # Landing Page + Channel combinations
            lp_channel_data = lp_channel_wow[lp_channel_wow['Week_Comparison'] == week_comp]
            if not lp_channel_data.empty:
                md_content.append("\n#### Top Landing Page + Channel Combinations\n")
                
                # Filter for significant traffic
                lp_channel_significant = lp_channel_data[lp_channel_data['Total users_current'] > 50]
                
                if not lp_channel_significant.empty:
                    top_channel_combos = lp_channel_significant.nlargest(5, 'Users_Change')
                    md_content.append("**Highest Growth Channel Combinations:**\n")
                    for _, row in top_channel_combos.iterrows():
                        parts = row['LP_Channel'].split(' | ')
                        md_content.append(f"- **{parts[1]}** → `{parts[0]}`: "
                                        f"{row['Users_Change']:+,.0f} users "
                                        f"({row['Users_Change_Pct']:+.1f}%)\n")
            
            md_content.append("\n---\n")
        
        # Key insights section
        md_content.append("## Key Insights\n")
        md_content.append(self._generate_key_insights(channel_wow, sm_wow, lp_wow))
        
        # Save markdown file
        output_file = self.output_dir / 'executive_summary.md'
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(''.join(md_content))
        print(f"Saved: {output_file}")    

    def _generate_key_insights(self, channel_wow, sm_wow, lp_wow):
        """Generate key insights section."""
        insights = []
        insights.append("\n### Traffic Trends\n")
        
        # Analyze overall channel trends
        channel_totals = channel_wow.groupby('Session Payscale Custom Channels').agg({
            'Users_Change': 'sum',
            'Key_Events_Change': 'sum'
        }).reset_index()
        
        # Top performing channels overall
        top_channels = channel_totals.nlargest(3, 'Users_Change')
        insights.append("**Strongest Performing Channels (Overall):**\n")
        for _, row in top_channels.iterrows():
            insights.append(f"- {row['Session Payscale Custom Channels']}: "
                          f"+{row['Users_Change']:,.0f} users, "
                          f"+{row['Key_Events_Change']:.0f} key events\n")
        
        # Engagement insights
        insights.append("\n### Engagement Patterns\n")
        
        # Find sources with best engagement improvements
        sm_engagement = sm_wow[sm_wow['Total users_current'] > 100].copy()
        if not sm_engagement.empty:
            top_engagement = sm_engagement.nlargest(3, 'Engagement_Change')
            insights.append("**Biggest Engagement Rate Improvements:**\n")
            for _, row in top_engagement.iterrows():
                insights.append(f"- {row['Session source / medium']}: "
                              f"{row['Engagement_Change']:+.2%} change in engagement\n")
        
        # Conversion insights
        insights.append("\n### Conversion Highlights\n")
        sm_conversions = sm_wow[sm_wow['Key events_current'] > 10].copy()
        if not sm_conversions.empty:
            top_conversions = sm_conversions.nlargest(3, 'Key_Events_Change')
            insights.append("**Top Key Event Increases:**\n")
            for _, row in top_conversions.iterrows():
                insights.append(f"- {row['Session source / medium']}: "
                              f"+{row['Key_Events_Change']:.0f} key events "
                              f"({row['Key_Events_Change_Pct']:+.1f}%)\n")
        
        return ''.join(insights)
    
    def run_analysis(self):
        """Run the complete analysis."""
        print("=" * 60)
        print("GA4 Week-over-Week Analysis")
        print("=" * 60)
        
        # Load and process data
        self.load_data()
        weeks = self.create_weekly_groups()
        
        if len(weeks) < 2:
            print("\nError: Need at least 2 weeks of data for comparison")
            return
        
        # Check for missing dates
        missing_dates_info = self.check_missing_dates(weeks)
        
        # Generate reports
        channel_wow = self.generate_channel_report(weeks)
        sm_wow = self.generate_source_medium_report(weeks)
        lp_wow = self.generate_landing_page_report(weeks)
        lp_source_wow = self.generate_landing_page_source_report(weeks)
        lp_channel_wow = self.generate_landing_page_channel_report(weeks)
        
        # Generate executive summary
        self.generate_executive_summary(channel_wow, sm_wow, lp_wow, lp_source_wow, lp_channel_wow, weeks, missing_dates_info)
        
        print("\n" + "=" * 60)
        print("Analysis complete!")
        print(f"All reports saved to: {self.output_dir.absolute()}")
        print("=" * 60)


if __name__ == "__main__":
    # Usage
    analyzer = GA4WeekOverWeekAnalyzer(
        csv_path='ga4_data.csv',  # Update with your CSV filename
        output_dir='output'
    )
    analyzer.run_analysis()

================
File: README.md
================
# GA4 Week-over-Week Analyzer

A Python tool for analyzing Google Analytics 4 (GA4) data on a week-over-week basis. This analyzer processes GA4 export data and generates comprehensive reports showing traffic trends, engagement patterns, and conversion insights across different channels, sources, and landing pages.

## Features

- **Week-over-Week Comparison**: Automatically groups data by week and calculates changes between consecutive weeks
- **Multi-Dimensional Analysis**: 
  - Channel performance
  - Source/Medium combinations
  - Landing page traffic
  - Landing page + Source/Medium combinations
  - Landing page + Channel combinations
- **Data Quality Checks**: Identifies missing dates within each week's data range
- **Executive Summary**: Auto-generated Markdown summary with key insights and trends
- **Detailed CSV Reports**: Exports granular data for further analysis

## Requirements

- Python 3.7+
- pandas
- numpy

## Installation

1. Clone or download this repository

2. Install required dependencies:
```bash
pip install -r requirements.txt
```

## Input Data Format

The script expects a CSV file exported from GA4 with the following structure:

- **Rows 1-6**: Comment/metadata rows (automatically skipped)
- **Row 7**: Header row with column names
- **Row 8**: Grand total row (automatically skipped)
- **Row 9+**: Daily data rows

### Required Columns

- `Date`: Date in YYYYMMDD format (e.g., 20240101)
- `Session Payscale Custom Channels`: Channel classification
- `Session source / medium`: Traffic source and medium
- `Page path and screen class`: Landing page URL
- `Total users`: Number of users
- `Engagement rate`: Engagement rate (decimal or percentage)
- `Key events`: Number of key events (conversions)
- `User key event rate`: Conversion rate per user

## Usage

### Basic Usage

1. Place your GA4 CSV export in the same directory as the script
2. Update the filename in the script:

```python
analyzer = GA4WeekOverWeekAnalyzer(
    csv_path='your_ga4_data.csv',  # Update this
    output_dir='output'
)
analyzer.run_analysis()
```

3. Run the script:
```bash
python ga4_analyzer.py
```

### Custom Configuration

```python
analyzer = GA4WeekOverWeekAnalyzer(
    csv_path='path/to/your/data.csv',
    output_dir='custom_output_directory'
)
analyzer.run_analysis()
```

## Output Files

All output files are saved to the `output/` directory (or your custom output directory):

### CSV Reports

1. **channels_week_over_week.csv**: Week-over-week changes by channel
2. **source_medium_week_over_week.csv**: Week-over-week changes by source/medium
3. **landing_pages_week_over_week.csv**: Week-over-week changes by landing page
4. **landing_page_source_week_over_week.csv**: Combined landing page + source/medium analysis
5. **landing_page_channel_week_over_week.csv**: Combined landing page + channel analysis

### Executive Summary

**executive_summary.md**: A comprehensive Markdown report including:
- Analysis period and data completeness status
- Top channel changes for each week comparison
- Source/medium performance highlights
- Landing page traffic trends
- Top-performing combinations
- Key insights on traffic trends, engagement patterns, and conversions

## Understanding the Reports

### Week Numbering

Weeks are numbered sequentially starting from Week 1. Weeks start on Monday and end on Sunday.

### Metrics Explained

- **Users_Change**: Absolute change in number of users
- **Users_Change_Pct**: Percentage change in users
- **Key_Events_Change**: Absolute change in conversions
- **Key_Events_Change_Pct**: Percentage change in conversions
- **Engagement_Change**: Change in engagement rate

### Data Completeness Warnings

The tool checks for missing dates within each week. If any dates are missing, warnings will appear in:
- Console output
- Executive summary report

Comparisons involving incomplete weeks should be interpreted with caution.

## Example Console Output

```
============================================================
GA4 Week-over-Week Analysis
============================================================
Loading data...
Initial shape: (245, 7)
Columns: ['Date', 'Session Payscale Custom Channels', ...]

Loaded 245 rows of data
Date range: 2024-01-01 to 2024-02-28

Found 8 weeks:
  Week 1: 2024-01-01 to 2024-01-07
  Week 2: 2024-01-08 to 2024-01-14
  ...

Checking for missing dates...
  Week 1: Complete (all 7 days present)
  Week 2: Missing 1 date(s) - 2024-01-10
  ...

Generating channel analysis...
Saved: output/channels_week_over_week.csv
...
============================================================
Analysis complete!
All reports saved to: /path/to/output
============================================================
```

## Troubleshooting

### "No valid data rows loaded!"

- Check that your CSV has the correct format
- Verify that Row 7 contains column headers
- Ensure date values are in YYYYMMDD format

### Date Parsing Errors

- Dates should be numeric (e.g., 20240101) or string format (e.g., "20240101")
- Check for any non-standard date formats in your export

### Missing Columns Error

Ensure your GA4 export includes all required columns listed in the "Required Columns" section above.

## License

This tool is provided as-is for analytics purposes.

## Contributing

Feel free to submit issues or pull requests for improvements.

================
File: requirements.txt
================
pandas>=1.3.0
numpy>=1.21.0



================================================================
End of Codebase
================================================================
